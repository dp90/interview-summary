{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import whisper\n",
    "import os\n",
    "model = whisper.load_model('large-v2')\n",
    "DATA_DIR = 'C:/src/interview-summary/data'\n",
    "SOURCE_FILE = os.path.join(DATA_DIR, '2min.wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\src\\interview-summary\\venv\\lib\\site-packages\\whisper\\transcribe.py:78: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Goedemiddag. Hoi, goedemiddag Angelique. Hoi. Ik heb op dit moment geen beeld. Ik weet niet wat jij nu voor je ziet. Ik zie jou. Ik krijg een zwart beeld bij jou staan. Ja, dat klopt. Omdat mijn vakje nog niet open is. Kijk. Dat kan ik ook niet zien. Dat praat toch wat makkelijker. Ja, precies. Lukt het een beetje zo op de digitale manier? Jawel, jawel. Ik ben opgegroeid in het digitaal, dus. Kijk. Jonge generatie, h√®. Kijk, helemaal super. Ja, fijn. Het is niet voor iedereen even makkelijk, dus zo fijn. Nee, dat is waar, dat is waar. Super. Goed dat je er bent. Nou. Ja, en ik had ook al eventjes gekeken naar de huiswerkopdrachten. Die heb je ook heel erg goed ingevuld. Bij voorbaat al dank. We gaan het daar ook over hebben. Ja. Dus, goed dat we er zijn. Ik zal even een korte introductie doen. En dan kunnen we het gesprek induiken. Mijn naam is Michael. En ik werk voor een onderzoeksbureau. En we zijn nu onderzoek aan het doen. Je had het waarschijnlijk al door. Naar ervaringen rondom fysiotherapie. Ja. En we zijn daar vooral echt op zoek naar jouw ervaring. Er zijn geen foute antwoorden. We willen echt graag weten wat jouw behoeften zijn rondom fysiotherapie. Ja. En we willen ook graag, als jij het goed vindt, het gesprek opnemen. Zodat we het later nog kunnen terugluisteren. Zodat we zeker weten dat we niks missen. Ja. Dus als dat voor jou ook akkoord is, dan... Ja, is goed. Super. Alles wat je vertelt wordt sowieso anoniem verwerkt.\n"
     ]
    }
   ],
   "source": [
    "result = model.transcribe(SOURCE_FILE)\n",
    "print(result[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import librosa\n",
    "from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor\n",
    "\n",
    "LANG_ID = \"nl\"\n",
    "MODEL_ID = \"jonatasgrosman/wav2vec2-large-xlsr-53-dutch\"\n",
    "\n",
    "processor = Wav2Vec2Processor.from_pretrained(MODEL_ID)\n",
    "align_model = Wav2Vec2ForCTC.from_pretrained(MODEL_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cpu'\n",
    "pipeline_type = \"huggingface\"\n",
    "align_model = align_model.to(device)\n",
    "labels = processor.tokenizer.get_vocab()\n",
    "align_dictionary = {char.lower(): code for char,code in processor.tokenizer.get_vocab().items()}\n",
    "\n",
    "align_metadata = {\"language\": result[\"language\"], \"dictionary\": align_dictionary, \"type\": pipeline_type}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import whisperx\n",
    "\n",
    "result_aligned = whisperx.align(result[\"segments\"], align_model, align_metadata, SOURCE_FILE, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storing words <> timestamps mapping in a file.\n",
    "import json\n",
    "\n",
    "with open('./word_ts.text', 'w+') as f:\n",
    "    for line in result_aligned['word_segments']:\n",
    "        line_temp = line.copy()\n",
    "        # WhisperX don't put a space after word but just to make sure.\n",
    "        line_temp['text'] = line_temp['text'].strip()\n",
    "        f.write(f'{json.dumps(line_temp)}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Settings for Nvidia\n",
    "import os\n",
    "import json\n",
    "\n",
    "diarize_manifest = {\n",
    "    'audio_filepath': f'./data/2min_reduced.wav',\n",
    "    'offset': 0,\n",
    "    'duration':  None,\n",
    "    'label': \"infer\",\n",
    "    'text': \"-\",\n",
    "    'num_speakers': None,\n",
    "    'rttm_filepath': f'./diarized/pred_rttms/audio_16k.rttm',\n",
    "    'uniq_id': \"\"\n",
    "}\n",
    "\n",
    "if not os.path.exists('./manifest.json'):\n",
    "    with open('./manifest.json', 'w') as f:\n",
    "        f.write(json.dumps(diarize_manifest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import wget\n",
    "from omegaconf import OmegaConf\n",
    "MODEL_CONFIG = os.path.join('./','diar_infer_meeting.yaml')\n",
    "if not os.path.exists(MODEL_CONFIG):\n",
    "    config_url = \"https://raw.githubusercontent.com/NVIDIA/NeMo/main/examples/speaker_tasks/diarization/conf/inference/diar_infer_meeting.yaml\"\n",
    "    MODEL_CONFIG = wget.download(config_url, './')\n",
    "\n",
    "config = OmegaConf.load(MODEL_CONFIG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "config.num_workers = 0\n",
    "config.batch_size = 32\n",
    "\n",
    "config.diarizer.manifest_filepath = './manifest.json'\n",
    "config.diarizer.out_dir = os.path.join('./', 'diarized')\n",
    "config.diarizer.speaker_embeddings.model_path = 'titanet_large'\n",
    "config.diarizer.speaker_embeddings.parameters.window_length_in_sec = [1.5, 1.0, 0.5]\n",
    "config.diarizer.speaker_embeddings.parameters.shift_length_in_sec = [0.75, 0.5, 0.25]\n",
    "config.diarizer.speaker_embeddings.parameters.multiscale_weights = [0.33, 0.33, 0.33]\n",
    "config.diarizer.speaker_embeddings.parameters.save_embeddings = False\n",
    "\n",
    "config.diarizer.ignore_overlap = False\n",
    "config.diarizer.oracle_vad = False\n",
    "config.diarizer.collar = 0.25\n",
    "\n",
    "\n",
    "config.diarizer.vad.model_path = 'vad_multilingual_marblenet'\n",
    "config.diarizer.oracle_vad = False # ----> Not using oracle VAD "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2023-01-27 16:16:11 clustering_diarizer:129] Loading pretrained vad_multilingual_marblenet model from NGC\n",
      "[NeMo I 2023-01-27 16:16:11 cloud:56] Found existing object C:\\Users\\david\\.cache\\torch\\NeMo\\NeMo_1.14.0\\vad_multilingual_marblenet\\670f425c7f186060b7a7268ba6dfacb2\\vad_multilingual_marblenet.nemo.\n",
      "[NeMo I 2023-01-27 16:16:11 cloud:62] Re-using file from: C:\\Users\\david\\.cache\\torch\\NeMo\\NeMo_1.14.0\\vad_multilingual_marblenet\\670f425c7f186060b7a7268ba6dfacb2\\vad_multilingual_marblenet.nemo\n",
      "[NeMo I 2023-01-27 16:16:11 common:912] Instantiating model from pre-trained checkpoint\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2023-01-27 16:16:11 modelPT:142] If you intend to do training or fine-tuning, please call the ModelPT.setup_training_data() method and provide a valid configuration file to setup the train data loader.\n",
      "    Train config : \n",
      "    manifest_filepath: /manifests/ami_train_0.63.json,/manifests/freesound_background_train.json,/manifests/freesound_laughter_train.json,/manifests/fisher_2004_background.json,/manifests/fisher_2004_speech_sampled.json,/manifests/google_train_manifest.json,/manifests/icsi_all_0.63.json,/manifests/musan_freesound_train.json,/manifests/musan_music_train.json,/manifests/musan_soundbible_train.json,/manifests/mandarin_train_sample.json,/manifests/german_train_sample.json,/manifests/spanish_train_sample.json,/manifests/french_train_sample.json,/manifests/russian_train_sample.json\n",
      "    sample_rate: 16000\n",
      "    labels:\n",
      "    - background\n",
      "    - speech\n",
      "    batch_size: 256\n",
      "    shuffle: true\n",
      "    is_tarred: false\n",
      "    tarred_audio_filepaths: null\n",
      "    tarred_shard_strategy: scatter\n",
      "    augmentor:\n",
      "      shift:\n",
      "        prob: 0.5\n",
      "        min_shift_ms: -10.0\n",
      "        max_shift_ms: 10.0\n",
      "      white_noise:\n",
      "        prob: 0.5\n",
      "        min_level: -90\n",
      "        max_level: -46\n",
      "        norm: true\n",
      "      noise:\n",
      "        prob: 0.5\n",
      "        manifest_path: /manifests/noise_0_1_musan_fs.json\n",
      "        min_snr_db: 0\n",
      "        max_snr_db: 30\n",
      "        max_gain_db: 300.0\n",
      "        norm: true\n",
      "      gain:\n",
      "        prob: 0.5\n",
      "        min_gain_dbfs: -10.0\n",
      "        max_gain_dbfs: 10.0\n",
      "        norm: true\n",
      "    num_workers: 16\n",
      "    pin_memory: true\n",
      "    \n",
      "[NeMo W 2023-01-27 16:16:11 modelPT:149] If you intend to do validation, please call the ModelPT.setup_validation_data() or ModelPT.setup_multiple_validation_data() method and provide a valid configuration file to setup the validation data loader(s). \n",
      "    Validation config : \n",
      "    manifest_filepath: /manifests/ami_dev_0.63.json,/manifests/freesound_background_dev.json,/manifests/freesound_laughter_dev.json,/manifests/ch120_moved_0.63.json,/manifests/fisher_2005_500_speech_sampled.json,/manifests/google_dev_manifest.json,/manifests/musan_music_dev.json,/manifests/mandarin_dev.json,/manifests/german_dev.json,/manifests/spanish_dev.json,/manifests/french_dev.json,/manifests/russian_dev.json\n",
      "    sample_rate: 16000\n",
      "    labels:\n",
      "    - background\n",
      "    - speech\n",
      "    batch_size: 256\n",
      "    shuffle: false\n",
      "    val_loss_idx: 0\n",
      "    num_workers: 16\n",
      "    pin_memory: true\n",
      "    \n",
      "[NeMo W 2023-01-27 16:16:11 modelPT:155] Please call the ModelPT.setup_test_data() or ModelPT.setup_multiple_test_data() method and provide a valid configuration file to setup the test data loader(s).\n",
      "    Test config : \n",
      "    manifest_filepath: null\n",
      "    sample_rate: 16000\n",
      "    labels:\n",
      "    - background\n",
      "    - speech\n",
      "    batch_size: 128\n",
      "    shuffle: false\n",
      "    test_loss_idx: 0\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2023-01-27 16:16:11 features:267] PADDING: 16\n",
      "[NeMo I 2023-01-27 16:16:11 save_restore_connector:243] Model EncDecClassificationModel was successfully restored from C:\\Users\\david\\.cache\\torch\\NeMo\\NeMo_1.14.0\\vad_multilingual_marblenet\\670f425c7f186060b7a7268ba6dfacb2\\vad_multilingual_marblenet.nemo.\n",
      "[NeMo I 2023-01-27 16:16:11 clustering_diarizer:156] Loading pretrained titanet_large model from NGC\n",
      "[NeMo I 2023-01-27 16:16:11 cloud:56] Found existing object C:\\Users\\david\\.cache\\torch\\NeMo\\NeMo_1.14.0\\titanet-l\\492c0ab8416139171dc18c21879a9e45\\titanet-l.nemo.\n",
      "[NeMo I 2023-01-27 16:16:11 cloud:62] Re-using file from: C:\\Users\\david\\.cache\\torch\\NeMo\\NeMo_1.14.0\\titanet-l\\492c0ab8416139171dc18c21879a9e45\\titanet-l.nemo\n",
      "[NeMo I 2023-01-27 16:16:11 common:912] Instantiating model from pre-trained checkpoint\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2023-01-27 16:16:11 modelPT:142] If you intend to do training or fine-tuning, please call the ModelPT.setup_training_data() method and provide a valid configuration file to setup the train data loader.\n",
      "    Train config : \n",
      "    manifest_filepath: /manifests/combined_fisher_swbd_voxceleb12_librispeech/train.json\n",
      "    sample_rate: 16000\n",
      "    labels: null\n",
      "    batch_size: 64\n",
      "    shuffle: true\n",
      "    time_length: 3\n",
      "    is_tarred: false\n",
      "    tarred_audio_filepaths: null\n",
      "    tarred_shard_strategy: scatter\n",
      "    augmentor:\n",
      "      noise:\n",
      "        manifest_path: /manifests/noise/rir_noise_manifest.json\n",
      "        prob: 0.5\n",
      "        min_snr_db: 0\n",
      "        max_snr_db: 15\n",
      "      speed:\n",
      "        prob: 0.5\n",
      "        sr: 16000\n",
      "        resample_type: kaiser_fast\n",
      "        min_speed_rate: 0.95\n",
      "        max_speed_rate: 1.05\n",
      "    num_workers: 15\n",
      "    pin_memory: true\n",
      "    \n",
      "[NeMo W 2023-01-27 16:16:11 modelPT:149] If you intend to do validation, please call the ModelPT.setup_validation_data() or ModelPT.setup_multiple_validation_data() method and provide a valid configuration file to setup the validation data loader(s). \n",
      "    Validation config : \n",
      "    manifest_filepath: /manifests/combined_fisher_swbd_voxceleb12_librispeech/dev.json\n",
      "    sample_rate: 16000\n",
      "    labels: null\n",
      "    batch_size: 128\n",
      "    shuffle: false\n",
      "    time_length: 3\n",
      "    num_workers: 15\n",
      "    pin_memory: true\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2023-01-27 16:16:11 label_models:126] Setting angular: true/false in decoder is deprecated and will be removed in 1.13 version, use specific loss with _target_\n",
      "[NeMo I 2023-01-27 16:16:11 features:267] PADDING: 16\n",
      "[NeMo I 2023-01-27 16:16:12 save_restore_connector:243] Model EncDecSpeakerLabelModel was successfully restored from C:\\Users\\david\\.cache\\torch\\NeMo\\NeMo_1.14.0\\titanet-l\\492c0ab8416139171dc18c21879a9e45\\titanet-l.nemo.\n"
     ]
    }
   ],
   "source": [
    "from nemo.collections.asr.models.msdd_models import ClusteringDiarizer\n",
    "\n",
    "model = ClusteringDiarizer(cfg=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2023-01-27 16:16:17 speaker_utils:92] Number of files to diarize: 1\n",
      "[NeMo I 2023-01-27 16:16:17 clustering_diarizer:303] Split long audio file to avoid CUDA memory issue\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "splitting manifest: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2023-01-27 16:16:19 vad_utils:100] The prepared manifest file exists. Overwriting!\n",
      "[NeMo I 2023-01-27 16:16:19 classification_models:247] Perform streaming frame-level VAD\n",
      "[NeMo I 2023-01-27 16:16:19 collections:296] Filtered duration for loading collection is 0.000000.\n",
      "[NeMo I 2023-01-27 16:16:19 collections:300] # 3 files loaded accounting to # 1 labels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "vad:   0%|          | 0/3 [00:00<?, ?it/s][NeMo W 2023-01-27 16:16:19 nemo_logging:349] c:\\src\\interview-summary\\venv\\lib\\site-packages\\torch\\amp\\autocast_mode.py:202: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "      warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n",
      "    \n",
      "vad: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:22<00:00,  7.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2023-01-27 16:16:41 clustering_diarizer:258] Converting frame level prediction to speech/no-speech segment in start and end times format.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "creating speech segments: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2023-01-27 16:16:42 clustering_diarizer:281] Subsegmentation for embedding extraction: scale0, ./diarized\\speaker_outputs\\subsegments_scale0.json\n",
      "[NeMo I 2023-01-27 16:16:42 clustering_diarizer:336] Extracting embeddings for Diarization\n",
      "[NeMo I 2023-01-27 16:16:42 collections:296] Filtered duration for loading collection is 0.000000.\n",
      "[NeMo I 2023-01-27 16:16:42 collections:300] # 148 files loaded accounting to # 1 labels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[1/3] extract embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:07<00:00,  1.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2023-01-27 16:16:50 clustering_diarizer:281] Subsegmentation for embedding extraction: scale1, ./diarized\\speaker_outputs\\subsegments_scale1.json\n",
      "[NeMo I 2023-01-27 16:16:50 clustering_diarizer:336] Extracting embeddings for Diarization\n",
      "[NeMo I 2023-01-27 16:16:50 collections:296] Filtered duration for loading collection is 0.000000.\n",
      "[NeMo I 2023-01-27 16:16:50 collections:300] # 223 files loaded accounting to # 1 labels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[2/3] extract embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:08<00:00,  1.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2023-01-27 16:16:58 clustering_diarizer:281] Subsegmentation for embedding extraction: scale2, ./diarized\\speaker_outputs\\subsegments_scale2.json\n",
      "[NeMo I 2023-01-27 16:16:58 clustering_diarizer:336] Extracting embeddings for Diarization\n",
      "[NeMo I 2023-01-27 16:16:58 collections:296] Filtered duration for loading collection is 0.000000.\n",
      "[NeMo I 2023-01-27 16:16:58 collections:300] # 449 files loaded accounting to # 1 labels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[3/3] extract embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 15/15 [00:10<00:00,  1.36it/s]\n",
      "[NeMo W 2023-01-27 16:17:09 speaker_utils:428] cuda=False, using CPU for eigen decomposition. This might slow down the clustering process.\n",
      "clustering: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:02<00:00,  2.19s/it]\n",
      "[NeMo W 2023-01-27 16:17:11 der:105] Check if each ground truth RTTMs were present in the provided manifest file. Skipping calculation of Diariazation Error Rate\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2023-01-27 16:17:11 clustering_diarizer:455] Outputs are saved in c:\\src\\interview-summary\\diarized directory\n"
     ]
    }
   ],
   "source": [
    "model.diarize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading timestamps <> Speaker Labels mapping\n",
    "\n",
    "speaker_ts = []\n",
    "with open('./diarized/pred_rttms/2min_reduced.rttm', 'r') as f:\n",
    "    lines = f.readlines()\n",
    "    for line in lines:\n",
    "        line_list = line.split(' ')\n",
    "        s = int(float(line_list[5]) * 1000)\n",
    "        e = s + int(float(line_list[8]) * 1000)\n",
    "        speaker_ts.append([s, e, int(line_list[11].split('_')[-1])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading words <> timestamps mapping, which we saved earlier\n",
    "import json\n",
    "\n",
    "word_ts = []\n",
    "with open('./word_ts.text', 'r+') as f:\n",
    "    for line in f:\n",
    "        line_temp = json.loads(line)\n",
    "        word_ts.append(line_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_ts_anchor(s, e, option='start'):\n",
    "  if option == 'end':\n",
    "    return e\n",
    "  elif option == 'mid':\n",
    "    return (s + e) / 2\n",
    "  return s\n",
    "\n",
    "def get_words_speaker_mapping(wrd_ts, spk_ts, word_anchor_option='start'):\n",
    "    s, e, sp = spk_ts[0]\n",
    "    wrd_pos, turn_idx = 0, 0\n",
    "    wrd_spk_mapping = []\n",
    "    for wrd_dict in wrd_ts:\n",
    "        ws, we, wrd = int(wrd_dict['start'] * 1000), int(wrd_dict['end'] * 1000), wrd_dict['text']\n",
    "        wrd_pos = get_word_ts_anchor(ws, we, word_anchor_option)\n",
    "        while wrd_pos > float(e):\n",
    "            turn_idx += 1\n",
    "            turn_idx = min(turn_idx, len(spk_ts) - 1)\n",
    "            s, e, sp = spk_ts[turn_idx]\n",
    "        wrd_spk_mapping.append({'word': wrd, 'start_time': ws, 'end_time': we, 'speaker': sp})\n",
    "    return wrd_spk_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "wsm = get_words_speaker_mapping(word_ts, speaker_ts, 'start')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_speaker = 0\n",
    "conversation = []\n",
    "segment = {'speaker': current_speaker, 'text': []}\n",
    "for word_dict in wsm:\n",
    "    if word_dict['speaker'] != current_speaker:\n",
    "        conversation.append(segment)\n",
    "        current_speaker = 0 if current_speaker == 1 else 1\n",
    "        segment = {'speaker': current_speaker, 'text': []}\n",
    "    segment['text'].append(f\"{word_dict['word']} \")\n",
    "conversation.append(segment)\n",
    "\n",
    "with open('./word_ts_final.txt', 'w+') as f:\n",
    "    for segment in conversation:\n",
    "        f.write(f\"Speaker {segment['speaker']}\\n\")\n",
    "        f.write(f\"{''.join(segment['text'])}\\n\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9 (tags/v3.10.9:1dd9be6, Dec  6 2022, 20:01:21) [MSC v.1934 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4ecf9c545552ead85d3401f7a23104da3ea6a7c7a71cd397c835eb854bb9b3aa"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
